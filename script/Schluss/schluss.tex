\section{Schluss}
Hier werden wir darauf eingehen was erreicht wurde was nicht und weshalb nicht.
\subsection{Anwendungsszenarien}
\subsectionauthor{Torben Brenner \& Lukas Seemann}
Der im Rahmen dieser Studienarbeit entwickelte Prototyp ist in der Lage, mithilfe der Erfassung von biometrischen Daten auf Emotionen zu schließen. Nun stellt sich die Frage, für welche Anwendungsszenarien die App nützlich sein könnte. Dieser Abschnitt soll diese Frage beantworten. \newline
\subsubsection{Studien}
\subsubsectionauthor{Torben Brenner \& Lukas Seemann}
Die Verwendung von Emotionserkennung über das Smartphone, ermöglicht es sehr einfach neue Studien in diesem Bereich durchzuführen. Durch das verteilen einer einfachen Installationsdatei könnten Studienteilnehmer ohne großen Aufwand an einer Studie teilnehmen. Denkbar wären hierbei Szenarios, bei denen die Teilnehmer zum Beispiel zehn Minuten täglich einen Test durchführen sollen oder in denen Sie durch eine bestimmte Auswahl von Sensoren dauerhaft analysiert werden. Damit wären Studien über den Emotionalen Verlauf eines Arbeitstags möglich.\newline
Unsere Architektur eignet sich insbesondere für solche Fälle, da es möglich ist eine beliebige Anzahl an Sensoren im Hintergrund arbeiten zu lassen und deren Ergebnisse zusammen zu führen. \newline 
Da die Emotion möglichst natürlich sein sollen und nicht bewusst vorgetäuscht beispielsweise durch Grimassen bei der Gesichtserkennung werden sollen, wäre es sinnvoll, die App zu dem Zeitpunkt einzusetzen, wenn die Emotion hervorgerufen wird. Es muss also unmittelbar die erste Reaktion des Anwenders auf Reize von außen eingefangen werden. Hierbei ergibt sich eine mögliche Anwendung der App emoTrix. \newline
Die App kann dazu eingesetzt werden, um zu bestimmen, wie Leute auf verschiedenes Bild- oder Filmmaterial reagieren. Während der Anwender verschiedene Bilder oder Filme zum ersten Mal sieht, müssen verschiedene Emotionstests der App gestartet werden. Am Ende kann man über die Auswertung nachvollziehen, welche Emotion bei welchem Bildmaterial empfunden wurden. Beispielsweise kann der Anwender, während er auf die Bilder reagiert, mit dem GSR-Sensor verbunden sein. Gleichzeitig wird bei jedem neuen Bild direkt bei der Reaktion ein Bild aufgenommen oder bei einem Film zum Beispiel alle 10 Sekunden. Diese Bilder können dann von der App analysiert werden. Zusätzlich kann der Anwender gebeten werden, das Gesehene zu kommentieren. Diese Sprachaufnahmen können auch mit der App analysiert werden. So ist es möglich, eine möglichst genaues Ergebnis der Emotionsbestimmung zu erhalten. \newline
Mithilfe der App kann im Nachhinein eine Studie durchgeführt werden, die untersucht, mit welchen Emotionen Menschen auf Bilder oder Filme reagieren. Hieraus können wertvolle Informationen und Statistiken gewonnen werden, was Menschen generell erfreut, bekümmert oder verärgert. \newline
Im Speziellen könnte auch eine Erweiterung stattfinden, in der Nutzer nicht nur analysiert werden sondern in denen diese dann auch die Auswertung überprüfen. So könnte im Auswertungsbildschirm eine Meldung von falschen Analysen mit eingebaut werden, um kontinuierlich Nutzer Feedback zu sammeln. In einer extrem ausgeweiteten Variante davon, könnte sogar ein neuronales Netz eingesetzt werden, um die Gewichtung von Indizien in Form der Kausalitätsregeln zu verbessern. Dabei würde das Training primär von den Nutzern durchgeführt. 
\subsubsection{Wirtschaft}
Auch in der Wirtschaft ist es interessant, was Kunden während dem Einkauf fühlen. Eine gängige Praxis in Call Centern ist zum Beispiel schon das prüfen des emotionalen Zustandes eines Anrufers. So können die Mitarbeiter im Zweifelsfall direkt deeskalierend mit dem Anrufer/-in sprechen.
\subsubsection{Unterstützung von Autisten}
\subsection{Fazit}

\subsection{Ausblick}
\subsectionauthor{Torben Brenner \& Lukas Seemann}
Da von den vorgestellten Erfassungsmöglichkeiten von biometrischen Daten bislang nur die Gesichtserkennung und die Hautleitwiderstand-Messung und damit Platz 1 und Platz 2 der Prioriserungsliste implementiert wurden, kann die App in Zukunft um weitere Emotionstest erweitert werden. Die hier vorgestellten Möglichkeiten können dafür als Vorlage dienen. Der nächste zu implentierende Emotionstest ist die Stimmerkennung. Danach folgt das Analysieren des Tippverhaltens und alternativ können im Anschluss daran auch noch die restlichen Erfassungsmöglichkeiten der Priorsierungsliste als Emotionstest umgesetzt werden. Neue Test können aufgrund der Architektur von emoTrix einfach integriert werden. \newline
Ein Erweiterung der App um Emotionstest führt zu genaueren Ergebnissen, da der Decider so mehr Daten als Entscheidungsgrundlage zur Verfügung hat. Dies ist auf jeden Fall eine wünschenswerte Weiterentwicklung der Anwendung.\newline
Zusärtlich müssen mit der Zeit auch die Kausalitätsregeln verbessert werden. \textbf{TORBEN: schreib was über deine KI Idee ;)} 
% Sollte mit KI Learning verbessert werden können 